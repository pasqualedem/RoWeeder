experiment:
  # It contains all the about the grids and the group of runs:
  name: RowWeed # name of the logger platform experiment
  group: Test # name of group of experiments for the logger platform
  continue_with_errors: False # continue with other runs even if a run fails
  start_from_grid: 0 # skip grids in the grid search
  start_from_run: 0 # skip runs from the selected grid
  search: grid
  # n_trials: 5

parameters:
  # Contains the parameters to build the grid.
  # Each value should be a dict or a list

  logger:
    wandb:
      offline: [False]
      entity: [cilabuniba]

  train_params:
    loss:
      class_weighting: [True]
      components:
        - focal:
            weight: 0.5
          contrastive:
            weight: 0.475
    seed: &seed [42] # random seed to set
    # substitution_threshold: [0.1] # threshold
    max_epochs: [2]
    compile: [False]
    initial_lr: [0.00001]
    optimizer: [AdamW]
    scheduler:
      type: [reduce_lr_on_plateau]
      step_moment: [epoch]
      patience: [0]
    watch_metric: [loss]
    freeze_backbone: [True]
    check_nan: [1] # check for nan every n batches

  model:
    name: [rw_segformer] # path to model class or model name contained in EzDL or super-gradients
    params:
      input_channels: &input_channels [['R', 'G', 'B']]

  dataset: # parameters depending on the class you defined for the dataset
    root: [dataset/patches/512] # path to the dataset
    gt_folder: [dataset/generated/07-02-2024_12:23:50] # path to the gt folder
    channels: *input_channels
    mean: [[0.485, 0.456, 0.406]]
    std: [[0.229, 0.224, 0.225]]

  dataloader:
    num_workers: [0]
    batch_size: [2]

other_grids:
